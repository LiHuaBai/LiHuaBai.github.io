{"meta":{"title":"Vince's Blog","subtitle":"You see the world, the world see you","description":null,"author":"毛靖文","url":"https://github.com/LiHuaBai","root":"/"},"pages":[{"title":"","date":"2019-03-20T06:48:44.000Z","updated":"2019-07-08T12:25:32.847Z","comments":false,"path":"tags/index.html","permalink":"https://github.com/LiHuaBai/tags/index.html","excerpt":"","text":""},{"title":"","date":"2019-03-20T06:49:01.000Z","updated":"2019-07-08T12:25:32.847Z","comments":false,"path":"categories/index.html","permalink":"https://github.com/LiHuaBai/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"19浙大计算机考研总结（二）","slug":"postgraduate2","date":"2019-03-23T03:30:00.000Z","updated":"2019-07-08T12:25:32.845Z","comments":true,"path":"2019/03/23/postgraduate2/","link":"","permalink":"https://github.com/LiHuaBai/2019/03/23/postgraduate2/","excerpt":"主要介绍一下初试4门课的复习方法，我的初试总分为399，各科成绩分布为数一139，英一82，政治68，408专业课109。","text":"主要介绍一下初试4门课的复习方法，我的初试总分为399，各科成绩分布为数一139，英一82，政治68，408专业课109。 总的来说其实考研复习并没有什么捷径和窍门，就是把该学的知识学完，记住，再通过做题提高熟练度，并归纳考试出题类型和做题方法，再有就是考试当天的发挥了。 数学复习用书数学其实相对来说知识点的内容是最少的，市面上主流的几个老师的资料都是没太大问题的，不过不要迷信某一个老师，要结合各个辅导资料的特点，有目的的去选用。我主要用的是张宇和李永乐的一些相关书籍，李永乐的偏基础，张宇会偏难一点，我想着汤家凤和张宇的好像都偏难，那我就选最难的好了，反正基础检验用李永乐的就好了。 一轮高数用的是十八讲，这书对于一轮复习来说是有些难了，几乎是做到自闭的程度，我做完一遍感觉自己大半题目都不会，于是一轮做了两遍，但从整个复习过程来看，十八讲配上张宇的基础班视频所讲到的题型知识点，基本包括了考研高数一年来做到的题型的80%，因此算是一本内容比较全的复习资料。 关于视频，我一开始其实是没看视频的，但自己一直看书的话，进度赶的比较快，没有停下来的时候，看了大半个月感觉好累，于是看了会视频，感觉确实会稍微轻松一点，调节一下心态，虽然看视频讲进度会稍微慢一点，但不至于出现自己连着学了两三章后，特别不想继续学的状态。同时有些重难点和自己理解比较有困难的地方，像中值定理的各个题型和解法等，看看视频会学的更快。 线性代数和概率论两门课本身内容比较少，考的也不难，因此不需要做太多的扩展。线代大家基本都是用李永乐的，对于考研哪些题目，李永乐的讲义基本够用了。概率论我选用了曹显兵的讲义，当时听说的是比张宇的简单，王式安的难，就抱着选个中等难道的心态选了这本书，总的来说内容也还可以。 暑假开始用了张宇1000题，B组题做的又挺自闭的，感觉自己一轮复习了这么久，居然正确率还是挺低的，于是B组也做了两遍。中间花了半个月去准备考了PAT，回来之后用660来检测一下自己的正确率和做题速度，也想来找点自信，确实感觉简单了不少，基本不到一周就做完了将近500题的选择题部分，正确率也还过得去。 真题与模拟题十月国庆回来开始做真题，做了几张96年-03年的卷子，大概了解了真题类型后，后面的卷子开始模拟考，大多数的卷子都能做到130+，而且其中07年到14年之前的卷子总体都很简单，每张卷子的70%的题目都是雷同的，14年之后的卷子新题目类型变多，同时每年的难度差距也变大了。 模拟卷我做过张宇、李永乐、李林和合工大超越卷，没做汤家凤的原因和前面一样，既然和张宇一样都是骗难的，那我难的部分就做最难的好了。张宇的卷子是90分常规简答题和张宇式难题的直接组合，做起来感觉各个题目间的难度差异和给人的感觉特别明显，但有些题目还是有查漏补缺的作用的，还有些题目可能过于追求考察某块特定的内容（比如为了考计算量，就刻意出一个计算量很大的，考察微分方程的各种类型，就刻意一道题需要用到三种微分方程类型变换），总的来说可以作为查漏补缺，但不适合模拟考。李永乐的题目还是比较偏重基础，难度也还正常，可以作为基础概念的检查，也可以模拟考。出于好奇心也做了两张李林的卷子，个人感觉基本都是常规简答题，即07-14年之间真题的那种，完全没有查漏补缺的感觉。关于押题的说法，我个人感觉营销水军多于实际，而且大多数也都是本身就未好好复习归纳的同学在说压到了，像压了一个定积分，有换元，分步积分，三角代换等常规方法，然后就说压中了，但实际上这本就应该是自己早已经归纳复习好的内容。因此好好的复习整理比押题要重要的多。合工大的超越卷的质量是最高的，给我一种高考数学卷的感觉，在有限的知识点上出的题目，基本都是需要看到题现场分析，没有现成的常规方法套用，很多时候为了模拟考，选择题都得靠凑答案来做。比较适合作为较难年份的模拟考。 今年数学虽然没有去年难，但总的来说也不算很简单的年份，我其实还是有两道不太会做的题目，其中一个求体积的三重积分大题完全没有思路，就写了个三重积分求体积公式，似乎也拿到了些过程分，因此遇到不会的题也不要完全空着，能想到的公式就都写上去。 英语单词其实英语本身算是我的弱势科目，我原来的英语水平也只是中等，大一的时候六级也是刚过没多少分。因此我一开始就把英语作为重难点科目来对待，把背单词作为初期复习最重要的任务，经常一早上就只干了背单词一件事。 关于单词书，5000-6000单词量的任意一本考研单词书都行，我用的是红宝书，个人不推荐朱伟的，他的教材和课程给我一种取巧的感觉，别人都需要背6000个，听朱伟的课就只要3000个单词就能考高分么，而且他把考研会考的熟词生意都提前给你标注出来，那之后做真题阅读的一半意义就没有了。 阅读关于阅读，我是没有做任何模拟题的，要说的话有做了一部分英语二的阅读。理解真题套路是阅读的重点，一开始可以看下唐迟的视频学习常规方法，但其实真的上考场了很多方法也不一定有用，最重要的还是理解文章，唐迟阅读的最终核心是选项服从文章主旨大意，这一点用上的可能较大。在平时自己做题的时候，为了训练自己分析题意的能力，我会反向做题，选出三个错误的选项，再分别标出是什么类型的错误，出卷人想从什么角度坑自己，这样可以更好利用每一道题。同时做错的题目也要分析是为什么错，是单词不认识，还是文章大意没看懂，还是有什么新的错误类型没有见过。良好的阅读成绩决定了你的下限，并且如果尽可能少错的话也能提高上限，我这次的考试估计也是错2道或1道。 作文关于作文，我由于专业课内容太多的缘故，虽然想早些看作文，但没一直没有时间。实际上正式花许多时间在作文上也是11月下旬的时候了，直接纯套写作模版肯定是不行的，每篇文章需要有具体的内容。但可以看作文书上的范文先积累一些词汇和语句，王江涛的作文我也有背了5，6篇，但也只是作为积累来背。之后可以参考各个作文书的写作方法和范文，自己针对各大题材，参考范围修改，写出自己的模版作文，并背出来用于考场上速写。主要的作文书我用的是王江涛和潘赟，结合王江涛的词句和潘越的九宫格方法来写自己的模版作文。 其他剩下的是新题型，完形和翻译，这些内容其实在早先年份都很难，但近几年难度在降低，因此大概看一些视频讲解的方法，自己练练真题也没别的特别的方法了，其中个人感觉新题型其实对最终成绩影像很大，排序题很容易改错一个选项，会多错个3，4题，因此全对和全错相差的其实可能并不是很多，做题的时候要慎重一点，也需要一些运气。并且我个人觉得可以在英语复习初期看一下早些年的翻译和作文真题，一方面初期除了背单词没别的其他事情，一方面看看翻译和背些作文范文，也有助于记单词和之后的阅读理解。 政治政治没太多内容，基本跟着肖秀荣来就不会有太大的问题，每年都会有说肖秀荣要被反押题了，不用太过在意。初期觉得政治资料太枯燥可以看看徐涛的视频，但主要还是得靠做题。最后时间多的话可以结合徐涛或其他老师人押题看看，没时间就只背肖四。我同样因为专业课408内容太多的缘故，政治也一直没什么时间，最后也只背了肖四，事实证明这样也基本够了。 408我用在专业课上的时间，可能是其他三门公共课的总和了，最终的成绩却还是一般，果然408还是难啊。4门专业课前期刚开始看的教材，有些不好理解的部分，找了些视频看，也没做题，尽快过完组原，OS和网络后就开始做天勤和王道。一开始做的是天勤，可能是本来就学的不好的缘故，第一遍做起来很困难，做完天勤后做王道感觉简单了许多。408的一个很大的问题就是，很多知识点是前后矛盾的，并且还有些知识点没有统一的说法，不同教材说法不一样，导致我很多地方一直处于矛盾之中。408的概念知识点数量基本和政治是一个数量级的，我是用了很多思维导图和笔记来记忆，这里推荐软件幕布。在10月份我每天花一个下午来一直背4门课的概念内容和考点。花了一个月才勉强记住大部分，同时导致我的政治选择题完全没时间背。 看考研群里的人很早就在讨论真题模拟的分数了，虽然王道每节的课后习题基本包括了所有真题，但我是在背下大多数内容前一直没有碰真题的卷子。我拿真题模拟基本是11月中旬的时候了。 最终考试的时候其实选择题做的还可以，最终成绩一般的原因是我错误估计了自己的数据结构，原来觉得自己DS学的最好，PAT也考了100，大题算法题写个暴力解法应该问题不大，结果今年两道大题都是要写算法+画图+分析的。一开始看第一大题，还在想最优算法，想了10分钟没写出来，想先做后面的在回来写暴力，结果第二题也很麻烦，又写了很久，也不确定是不是一定对，做后面的题目心态就有点不好了。因此果然还是不能轻视每一科的复习。 其他建议（结合个人情况） 考研复习要有自己的计划，不要太在意别人的进度，也不要水群，自己每一个阶段的安排都要有目的性，比如要记住80%的单词，掌握常规的高数题解方法 先将目标分数定高点总没错，想考380就定400分为目标，同时计划也尽可能定早些，留些时间出来，总是会有意料之外的状况 如果不是能够激励自己的研友，还是一个人学习吧 结合自己的习惯安排作息，和复习方式。比如暑期是否要留校，平时几点开始复习等。由于我个人情况比较特殊，自控能力还过得去，暑假在家的两个月是复习效率最高的两个月，但是对于大多数人来说，在学校图书馆之类的可能会更好。而且我感觉早起比较困难，同时起得很早的话一天的学习都比较困，因此我一般都是9点多出门开始复习一直到晚上11：30。 可以用一些计时软件来记录每天学习的时长，同时手机可以断网或者开飞行模式以免分心。计时主要是为了检查长期的复习效果（比如自己一个月来欠了多少复习时长），我给自己的目标是日均复习时长10个小时，即使出现其他的事也要算上，比如返校那天可能要坐车收拾东西没法学，就从其他日子里补上。我暑假期间总计时日均是超过10小时的，开学后由于事情变多逐渐跟不上，从7月中旬到12月初，我的总共学习时长是1200+小时，总共大概欠了8，90个小时吧。但有些人可能本身就足够专注，不会记得计时，因此不一定适合所有人 一周可以休息个半天这样，因为长时间的专注学习确实无法一直保持，需要一些放松的时候。","categories":[{"name":"日常","slug":"日常","permalink":"https://github.com/LiHuaBai/categories/日常/"}],"tags":[]},{"title":"19浙大计算机考研总结（一）","slug":"postgraduate1","date":"2019-03-23T02:10:00.000Z","updated":"2019-07-08T12:25:32.844Z","comments":true,"path":"2019/03/23/postgraduate1/","link":"","permalink":"https://github.com/LiHuaBai/2019/03/23/postgraduate1/","excerpt":"这将近一年的考研，也算是本科期间所做的持续时间最长的事情了，最终侥幸上岸了，这也算是这些年来为数不多的向好的方向的重大变化了。","text":"这将近一年的考研，也算是本科期间所做的持续时间最长的事情了，最终侥幸上岸了，这也算是这些年来为数不多的向好的方向的重大变化了。 写在前面其实最终决定要考研也是大三上才决定的，当时本来还抱着抓紧学些机器学习和图像方面东西的心态，希望在毕业前能够找到一份算法岗，然而算法这东西，一旦看起来，就给人一种漫无边际的感觉，学了一个多月，发现也只学了一点基础，而且实践起来也很是困难。 这时感觉自己抱着这种不确定能否掌握的心态可能学不下去了，于是便决定考研，让自己的目标更加明确，要做的事，要做到什么程度，也更加明确。 有些人说考研相比起找工作和出国，太过惨烈，只有一个志愿，近一年的准备时间，只有一次考试的机会，，一旦出错双非的学生根本没有调剂的退路，一年来的复习时间也基本算是浪费掉了。 这样说确实没错，但我却觉得也可以从另一个角度来看，正是凭借着考研，才能有机会进入到各大高校，只要考的分数够高，在自身科研素养还过得去的情况下，清北也是有机会上的，就相当于所有的985211顶尖高校都给了我们一个机会，哪怕是双非的学生也可以去尝试的机会。 我也想借此与高校的学生来公平竞争一下，来衡量一下自己经过这几年的功夫，与985学生们的学习与抗压能力的差距究竟是拉大了，还是缩小了。 除去个人发展的打算外，支撑着我一年来的复习准备的另一个原因是大学这几年来总是能够看到在名校学习的高中同学的日常吧，他们有着更好的学习环境，也有更多出国交流的机会，同时还能有着自己的许多兴趣生活。想到自己一直迫于本科学历的劣质劣势直在一堆所谓的项目和比赛中度过了这些年，似乎大学生活过于单一了些。复习到9，10月份，又一批浙大、北大、华科等学校的同学纷纷轻松保研，更让我确定自己想要通过考研来进入向往的高校。 目标学校CS目前无论在什么学校，都是热门，起初也考虑过同济，中山等看上去分数没有这么爆炸的学校，然而总是会听到各种传闻，XX大学今年CS要炸了，似乎选择那些学校也并不能带给我多少安心感，于是想不如还是定更好的学校。 我主要在浙大，上交，南大三个学校里考虑，各有不同优势，南大机器学习最强，交大在上海的位置更有利发展，浙大则是比较近，各类消息比较好找。最终觉得浙大的口碑更为好（没有歧视双非的消息），同时在杭州读大学，之前也已经有过PAT的相关准备，PAT相当于提前给一次复试的机会，其实是比较像高考的自主招生的感觉。于是就顶着2000+的报名人数头铁报了浙大，不过正好我9月份的PAT考了100分，多少让自己看到了一些希望。 关于心态调整自从打算报考浙大之后，其实一直都很没有安全感，完全无法想象自己考上的样子，不过也让自己不敢停下来复习，基本稍一休息就会担心自己日后没考上而感到后悔，便只好尽可能的利用时间复习。 有时会看看网上的其他考研小伙伴，有些人确实十分励志，也侧面激励着我。每晚睡觉前也会给自己打打鸡血，我相信一定有人通过自己的努力，活成了自己理想中的样子。以前看到往年的学长的话，觉得和自己的心态很像。因为很多东西在决定考研的时候已经想好了，所以虽然害怕，但是也会一直坚持下来不放弃。哪怕自己最后失败了，那我也尝试过了，既然技不如人，说明自己的水平不过如此，之后再去选择其他的发展方向。 不必后悔 总结考研是一场实力、心态、运气的较量，实力是由天赋和努力决定的，而其实到了大学这个阶段，早已过了比天赋的时候，不同人之间的差距更多的是由之前的知识基础来决定，但即便如此，本科的基础差距也依旧可以根据后天的努力来弥补的，即这一年来的复习准备。 所以不要因为自己过去的种种失败而妄自菲薄。实力决定了自己的下限，虽然运气也是很重要的因素，但运气会伴随自己的一生，是早已经注定的，所以只要尽力去做了，不用太过在意运气的好坏。认真评估一下自己能否做到坚持，能否做到专注，能否做到自信，不要因为别人的话语而动摇自己的计划，也祝愿之后的学弟学妹都能金榜题名。","categories":[{"name":"日常","slug":"日常","permalink":"https://github.com/LiHuaBai/categories/日常/"}],"tags":[]},{"title":"重开博客采坑记录","slug":"Blog_Git_Experience","date":"2019-03-19T07:20:13.000Z","updated":"2019-07-08T12:25:32.843Z","comments":true,"path":"2019/03/19/Blog_Git_Experience/","link":"","permalink":"https://github.com/LiHuaBai/2019/03/19/Blog_Git_Experience/","excerpt":"","text":"再次利用hexo搭建个人博客的过程中，依旧踩了很多坑，主要之前Git用的不熟练，现在顺便也补一下Git的配置和使用方法。 Git主机秘钥配置使用Git来进行分布式管理，首先要配置用户名，邮箱，以及主机秘钥12git config --global user.name \"yourname\"git config --global user.email \"youremail\" 邮箱与用户名与github一致ssh-keygen -t rsa -C &quot;youremail&quot;会让设置密码什么的，一般可以直接回车不设置，完成后在~.ssh/文件夹里，会有公钥id_rsa.pub和私钥id_rsa，公钥在github在github的账号settings的SSH keys处配置。 两个github账号引起的错误想着本来那个账号注册的比较随便，用户名有点怪怪的，于是打算换一个，结果换了一个配置好秘钥后居然不能push到仓库也不能用hexo deploy，原因是第一次用git提交到远程仓库时，需要输入github的账号密码，系统自动记住了，换账号后，需要主要删除之前账号的信息，在控制面板-&gt;用户账户-&gt;凭据管理器（管理windows凭据）处，删除git凭据 hexo博客搭建各类坑原先开启标签和分类后，切换主题后再切回来会出错，导致主题样式异常，需要重新开启标签和分类 所用主题开启标签和分类后，还需要去改标签和分类的index.md的内容，才能正确显示 评论系统，开启gitment原先是这样的123456gitment: false# gitment:# owner: LiHuaBai# repo: LiHuaBai.github.io# client_id:# client_secret: 正确开启写法123456# gitment: falsegitment: owner: LiHuaBai repo: LiHuaBai.github.io client_id:申请的id client_secret:申请的secret 在此处申请OAuthid其中的Homepage URL和Authorization callback URL可以都写成https://username.github.io申请后的OAuth Apps在这里查看 然而配置好依旧不能用，查找后发现是原开发者的服务器已经关闭了，找了一个别人搭好的服务器，在主题的layout文件夹里面找到gitment的部分，修改如下12&lt;link rel=\"stylesheet\" href=\"https://jjeejj.github.io/css/gitment.css\"&gt;&lt;script src=\"https://jjeejj.github.io/js/gitment.js\"&gt;&lt;/script&gt; 可正常使用评论功能 github存储hexo本地文件为了之后换电脑啥的博客不在丢失，觉得最好还是把本地文件存在github上，这样也容易找回，一般来说可以用同一个仓库，新建一个分支，先把仓库中所存储的博客的静态html文件clone下来git clone git@github.com:LiHuaBai/LiHuaBai.github.io.git只留下git文件，其余删除，把hexo文件夹内部的以下文件复制过来 scaffords source themes .gitignore _conig.yml package.json然后将这些文件保存在仓库的另一个分支下1234git checkout -b hexo_local_filegit add --allgit commit -m &quot;save local file&quot;git push --set-upstream origin hexo 利用github重新恢复博客将hexo文件从仓库中clone下来，重新安装依赖库后，就可以重新启动了1234git clone -b hexo_local_file git@github.com:LiHuaBai/LiHuaBai.github.io.gitcnpm installhexo ghexo s","categories":[{"name":"采坑","slug":"采坑","permalink":"https://github.com/LiHuaBai/categories/采坑/"},{"name":"Git","slug":"采坑/Git","permalink":"https://github.com/LiHuaBai/categories/采坑/Git/"}],"tags":[]},{"title":"于是我又重开了这个坑","slug":"New_Start","date":"2019-03-19T04:05:02.000Z","updated":"2019-07-08T12:25:32.844Z","comments":true,"path":"2019/03/19/New_Start/","link":"","permalink":"https://github.com/LiHuaBai/2019/03/19/New_Start/","excerpt":"","text":"时隔两年，考研结束后的我本想捡回之前的博客（虽然已经闲置一年），兴致满满的一通操作，然而发现1年前电脑重装过后当时的本地hexo文件并没有保存备份，github上只存着静态html，md文件也没保存。不过想想在原来博客上也没写多少东西，尝试着恢复了一些内容后，于是，我又重开了一个坑。 起因两年前也只是一时脑子一热，觉得有个个人博客还挺帅的，作为一个CS专业的学生，自诩动手能力强，便直接上手搭建了之前的博客（然而都是靠学长的帮助，我似乎并没有干多少事情）。 本以为自己之后一定会做出很多厉害的项目，经常更新博客，实际上过了起初的劲头，也逐渐懒得打开博客，同时自己发现学的和写的东西也都只是网上到处都有的入门级代码，陷入了一种好像没有什么特别值得写的东西的这样莫名的心态。 再加上考研复习那一年也就基本没怎么也代码了，本来都已经忘了博客这回事了，但我似乎是看到了什么事便不能随便忘掉的那种人，再加上最近了解了以前同学的情况和学长们的博客，发现大家都好像还挺努力地过着自己的人生。 虽然自己想做的事情似乎有很多结果都不满意，但也正是因为做了许多事情，才能走到今天这步。于是我又很不要脸的重新拉了一个博客。 目标每次想到人生目标这类东西，我总觉得太过遥远，就感觉自己大一时候那个什么都不懂的大学新生状态还历历在目，只是因为害怕未知的世界就只好努力学习，假装自己目标很明确的样子。 感觉自己其实兴趣比较广，也有看起来还可以的规划，但却又习惯性去担心自己的目标是不是正确，每当达成了一点东西后，高兴了几天后，又继续担心之后的任务是不是正确，毕竟越走越远，花的时间精力也越来越多，只能硬着头皮继续前进了。 既然已经考上了ZJU的研究生，也总算有了一点学校的优势，接下来的就得让自己在这个更高的平台上有所成就，希望能在硕士阶段较好的融入ZJU的氛围（大佬遍地的感觉真令人感到不安），顺利的发一些paper，目前来说还是做之后找工作的打算的。 目前的状态大学四年，我发现自己确实迫于生计把大多数时间都用在学业上了，以至于现在回想起来，个人兴趣爱好方面的东西在这几年几乎没有任何增长，课外阅读量甚至没有高中三年多。并且有了上一个博客的经验，技术内容确实可能没有那么多可写的，也希望自己趁这段时间多去尝试一些别的事情，多出去逛逛，可能的话也想学一下新的语言之类的，也该丰富一下自己的生活了。","categories":[{"name":"日常","slug":"日常","permalink":"https://github.com/LiHuaBai/categories/日常/"}],"tags":[]},{"title":"基于LSTM的问答匹配实践","slug":"LSTM_QA_match","date":"2019-02-26T16:00:00.000Z","updated":"2019-07-08T12:25:32.844Z","comments":true,"path":"2019/02/27/LSTM_QA_match/","link":"","permalink":"https://github.com/LiHuaBai/2019/02/27/LSTM_QA_match/","excerpt":"","text":"之前的博客里，都写了一些tensorflow的入门学习，当时在学习tensorflow时一个让我很头疼的问题就是，只要自己写大点深点的网络，基本都是跑不动，损失函数降了一阵就不动了，估计可能是初始化，正则防止过拟合没做好。这个LSTM网络算是我难得写出来能跑的网络，现在有空了来记录一下。 LSTM网络LSTM网络是由RNN变化而来，当时一直很困惑的问题就是关于RNN，横的算一层还是竖的算一层，应该是横的算一层，因此最简单的RNN即是一层隐藏层，光看RNN模型图的话，总会有些误解，实际上RNN每个单元的输入对应的状态隐藏层是可以有多个输出的，因为一个单元的输入维度也是n维，对应隐藏层的输出若是m维，则但看这个单元可理解为n到m的全连接网络。这个隐藏层维度一般就是由自己设定。 同时关于RNN和LSTM的输出，一般在理解模型时，都是以一个句子为例，只需要最后一个序列单元对应的输出，实际上通过tensorflow和keras框架得到的输出，是包括所有单元的，如RNN的输出outputs, last_state = tf.nn.static_rnn(cell, inputs)理论上可认为outputs[-1]与last_state是相等的 关于LSTM，由于其状态由两个值构成(Ct,Ht)，认为其状态是一个tuple，其输出为outputs, state_out = tf.contrib.rnn.static_rnn(cell, inputs, initial_state=tuple_state)这样得到的state_out包含两个值，若所写的网络中间有多个隐藏层，则可以有更多状态输出。 同时对于双向LSTM，其输出outputs的每个单元尺寸为两个LSTM的隐藏层输出单元的和 LSTM网络代码123456789def biLSTMCell(x, hiddenSize): input_x = tf.transpose(x, [1, 0, 2])# 0 1 2 变成1 0 2，最外面两个维度转置 input_x = tf.unstack(input_x) #多维变成低维，默认axio = 0，最外层的维度 lstm_fw_cell = tf.contrib.rnn.BasicLSTMCell(hiddenSize, forget_bias=1.0, state_is_tuple=True) lstm_bw_cell = tf.contrib.rnn.BasicLSTMCell(hiddenSize, forget_bias=1.0, state_is_tuple=True) output, _, _ = tf.contrib.rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, input_x, dtype=tf.float32) output = tf.stack(output) #一维变成多维 output = tf.transpose(output, [1, 0, 2]) return output 需要使用unstack与stack函数变化矩阵维度结构，unstack将一个整体tf变量变成了一个数字list内部的多个tf量，作为输入进入网络输出后，再使用stack函数变回整体tf变量。完整代码","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"https://github.com/LiHuaBai/categories/DeepLearning/"},{"name":"tensorflow","slug":"DeepLearning/tensorflow","permalink":"https://github.com/LiHuaBai/categories/DeepLearning/tensorflow/"}],"tags":[]},{"title":"爬虫入门（一）","slug":"spider1","date":"2017-09-04T16:00:00.000Z","updated":"2019-07-08T12:25:32.846Z","comments":true,"path":"2017/09/05/spider1/","link":"","permalink":"https://github.com/LiHuaBai/2017/09/05/spider1/","excerpt":"一直以来都觉得爬虫是个很神奇的东西，既然已经学了Python，就准备自己来写写看","text":"一直以来都觉得爬虫是个很神奇的东西，既然已经学了Python，就准备自己来写写看 要用到的库123import urllib.requestimport urllib.parseimport re 在Python3较后的版本里，urllib库被封装在urllib.request，功能基本是类似的。 实现原理爬虫的根本原理是模拟浏览器的行为，从而获取数据，然而现在web技术发展较快，浏览器也有不同的方法来得到数据，我看了些资料，主要就找到两种较常见的方法，一是后端静态加载的网页，通过获取其html源码来得到数据，二是利用JS异步请求动态加载的，通过请求得到JSON数据。 我首先试着获取静态加载的网页，在这里选择了58同城的二手房页面。可以在chrom浏览器开发者工具（F12）的network里看到网址信息 浏览器信息 模拟浏览器的所需的信息在Request Hearders（作为请求头）和Query String Parameters（作为请求发送的数据）123456789101112131415161718# url地址，这里选择了江干区二手房的第三页，不过其后天数据有随机性，并不固定url=&apos;http://hz.58.com/jianggan/ershoufang/pn3/?&apos;data=&#123;&#125;head=&#123;&#125;# 请求头head[&apos;User-Agent&apos;]=&apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.78 Safari/537.36&apos;# 请求数据data[&apos;utm_source&apos;]=&apos;sem-sales-baidu-pc&apos;data[&apos;utm_campaign&apos;]=&apos;sell&apos;data[&apos;utm_medium&apos;]=&apos;cpc&apos;data[&apos;spm&apos;]=&apos;62854932425.16537920598&apos;data[&apos;PGTID&apos;]=&apos;0d300000-0000-0a0c-8067-a60b6ee48510&apos;data[&apos;ClickID&apos;]=1data=urllib.parse.urlencode(data).encode(&apos;utf-8&apos;)req=urllib.request.Request(url,data,head,method=&apos;GET&apos;)# 将请求得到的响应（response）下载下来req = urllib.request.urlopen(req)my_data = req.read() 在获得了html源码数据后，如何提取所需的内容，通常是利用正则表达式，这对于没认真学正则的我来说还是挺头疼的，主要是要提取标签内的数据，同时对于byte，str，list，obj，dict等数据结构要会转换运用，才能得到想要的内容。 完整代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import urllib.requestimport urllib.parseimport re#保存HTML源码def saveFile(data): path = &quot;D:\\\\python\\\\Spider\\\\58\\\\html.out&quot; f = open(path,&apos;wb&apos;) f.write(data) f.close()#所需要的数据，考虑到正则匹配的水平太差，就挑了好拿的部分content = &#123;&#125;content[&apos;tingshi&apos;] = []content[&apos;daxiao&apos;] = []content[&apos;chaoxiang&apos;] = []content[&apos;louceng&apos;] = []content[&apos;jiedao&apos;] = []content[&apos;price&apos;] = []name = [&apos;tingshi&apos;,&apos;daxiao&apos;,&apos;chaoxiang&apos;,&apos;louceng&apos;,&apos;jiedao&apos;]#正则匹配式res_tr1 = r&quot;&lt;ul class=&apos;house-list-wrap&apos;&gt;(.*?)&lt;/ul&gt;&quot;res_tr2 = r&quot;&lt;span&gt;(.*?)&lt;/span&gt;&quot;res_price = r&apos;&lt;b&gt;(.*?)&lt;/b&gt;&apos;res_jiedao = r&apos;&lt;a&gt;(.*?)&lt;/a&gt;&apos;def getdata(datalist): num = -1 for x in datalist: num = num + 1 num = num%6; if num == 5: continue if num &lt; 4: print(x) content[name[num]].append(x) if num == 4: content[name[num]].append(re.findall(res_jiedao,x,re.S|re.M))url=&apos;http://hz.58.com/jianggan/ershoufang/pn3/?&apos;data=&#123;&#125;head=&#123;&#125;head[&apos;User-Agent&apos;]=&apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.78 Safari/537.36&apos;data[&apos;utm_source&apos;]=&apos;sem-sales-baidu-pc&apos;data[&apos;utm_campaign&apos;]=&apos;sell&apos;data[&apos;utm_medium&apos;]=&apos;cpc&apos;data[&apos;spm&apos;]=&apos;62854932425.16537920598&apos;data[&apos;PGTID&apos;]=&apos;0d300000-0000-0a0c-8067-a60b6ee48510&apos;data[&apos;ClickID&apos;]=1data=urllib.parse.urlencode(data).encode(&apos;utf-8&apos;)req=urllib.request.Request(url,data,head,method=&apos;GET&apos;)req = urllib.request.urlopen(req)my_data = req.read()saveFile(my_data)#匹配出ul标签内数据my_strdata = bytes.decode(my_data)m_list1 = re.findall(res_tr1,my_strdata,re.S|re.M)m_tr1 = &quot;&quot;.join(m_list1)content[&apos;price&apos;] = re.findall(res_price,m_tr1,re.S|re.M)m_list2 = re.findall(res_tr2,m_tr1,re.S|re.M)getdata(m_list2)print(content) html源码 提取逐个数据 在实际操作中，还是出现了一些状况，对于ul标签内的每一份二手房数据，部分会缺失一部分信息（如建造时间，地铁距离未标明），在提取的时候还是会导致出错，由于该网站后台提供数据是有一定随机性的，尝试几次后，有时能完整提取，有时候就会因为数据的不完整性，导致爬取格式出错。","categories":[{"name":"Python","slug":"Python","permalink":"https://github.com/LiHuaBai/categories/Python/"}],"tags":[]},{"title":"tensorflow学习笔记（三）——前馈神经网络","slug":"tf3","date":"2017-08-17T16:00:00.000Z","updated":"2019-07-08T12:25:32.846Z","comments":true,"path":"2017/08/18/tf3/","link":"","permalink":"https://github.com/LiHuaBai/2017/08/18/tf3/","excerpt":"对于机器学习而言，神经网络算是必学的内容了，也试着用tensorflow框架写了一下","text":"对于机器学习而言，神经网络算是必学的内容了，也试着用tensorflow框架写了一下 神经网络神经网络即模仿生物学中的神经细胞进行接受信号，传播信号的过程。对于单个神经细胞，可能有n个输入以及m个输出，但对于复杂的神经网络，有多层神经网络，其中接受原始信号的为输入层，接受来自前面神经元输出的叫做隐藏层，隐藏层可以有多层，最后一层即是输出层。而前馈神经网络是一种最简单的神经网络，各神经元分层排列。每个神经元只与前一层的神经元相连。接收前一层的输出，并输出给下一层．各层间没有反馈。 实现原理对于前馈神经网络的实现，利用的是教材中所给的数据input_data，为带有数字的图片。123# 可在tensorflow库中导入from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets(&apos;MNIST_data&apos;, one_hot=True) 神经网络模型的构建1234567891011121314151617181920212223242526# 首先定义设置网络层的函数，输入输出间均为线性关系def set_layer(inputs,in_size,out_size,layer_name,activate_function = None): # 权重和偏差量定义 W = tf.Variable(tf.random_uniform([in_size, out_size], -1.0, 1.0), name=&quot;W&quot; + layer_name) bias = tf.Variable(tf.constant(0.1, shape=[out_size]), name=&quot;bias_&quot;+ layer_name) # 线性模型 Wx_Plus_b = tf.matmul(inputs, W) + bias # 防止过拟合,keep_prob为每个元素的被保留概率 Wx_Plus_b = tf.nn.dropout(Wx_Plus_b, keep_prob) # 激活函数的使用 if activate_function is None: outputs = Wx_Plus_b else: outputs = activate_function(Wx_Plus_b) return outputs# 原始数据占位符xs = tf.placeholder(tf.float32, [None, n_input], name=&quot;input&quot;)ys = tf.placeholder(tf.float32, [None, n_classes], name=&quot;output&quot;)# 保留概率keep_prob = tf.placeholder(tf.float32)# 神经网络的构建h1 = set_layer(xs, n_input, hidden_units1, &apos;hidden_layer_1&apos;, activate_function=tf.nn.tanh)# softmax和tf.nn.tanh为激活函数，都可用,常见的还有relu函数h2 = set_layer(h1, hidden_units1, hidden_units2, &apos;hidden_layer_2&apos;, activate_function=tf.nn.tanh)prediction = set_layer(h2, hidden_units2, n_classes, &apos;prediction_layer&apos;, activate_function=None)# 预测输出在用None无激活函数效果较好，也可用tf.nn.softmax，tf.nn.tanh等激活函数 训练指标的设置12345678910111213# cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction)))# 有看到资料说和下面一句意义相同，但实际上无法使用，# tf.nn.softmax_cross_entropy_with_logits将预测值转化为总和为一的概率值，并与实际值的计算交叉熵代价函数# tf.reduce_mean取均值cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=ys, logits=prediction))# 训练目标为使cross_entropy最小，使用梯度下降法进行训练train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)tf.summary.scalar(&apos;loss&apos;, cross_entropy)# 准确率correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(ys, 1))# 得到true false数据accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))# 求均值 完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_dataimport timemnist = input_data.read_data_sets(&apos;MNIST_data&apos;, one_hot=True)def set_layer(inputs,in_size,out_size,layer_name,activate_function = None): W = tf.Variable(tf.random_uniform([in_size, out_size], -1.0, 1.0), name=&quot;W&quot; + layer_name) bias = tf.Variable(tf.constant(0.1, shape=[out_size]), name=&quot;bias_&quot;+ layer_name) Wx_Plus_b = tf.matmul(inputs, W) + bias Wx_Plus_b = tf.nn.dropout(Wx_Plus_b, keep_prob)#防止过拟合,keep_prob为每个元素的被保留概率 if activate_function is None: outputs = Wx_Plus_b else: outputs = activate_function(Wx_Plus_b) return outputs# 参数设定hidden_layers = 1hidden_units1 = 200hidden_units2 = 50n_input = 784n_classes = 10learning_rate = 0.8# 神经网络的构建xs = tf.placeholder(tf.float32, [None, n_input], name=&quot;input&quot;)ys = tf.placeholder(tf.float32, [None, n_classes], name=&quot;output&quot;)keep_prob = tf.placeholder(tf.float32)h1 = set_layer(xs, n_input, hidden_units1, &apos;hidden_layer_1&apos;, activate_function=tf.nn.tanh)h2 = set_layer(h1, hidden_units1, hidden_units2, &apos;hidden_layer_2&apos;, activate_function=tf.nn.tanh)prediction = set_layer(h2, hidden_units2, n_classes, &apos;prediction_layer&apos;, activate_function=None)cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=ys, logits=prediction))train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)tf.summary.scalar(&apos;loss&apos;, cross_entropy)# 训练结果准确性correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(ys, 1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))# 训练init = tf.initialize_all_variables()n_epochs = 40batch_size = 100with tf.Session() as sess: st = time.time() write = tf.summary.FileWriter(&apos;logs/&apos;, sess.graph) sess.run(init) for epoch in range(n_epochs): n_batch = int(mnist.train.num_examples / batch_size) for i in range(n_batch): batch_xs, batch_ys = mnist.train.next_batch(batch_size) sess.run(train_op, feed_dict=&#123;xs: batch_xs, ys: batch_ys, keep_prob:0.75&#125;) print (&apos;epoch&apos;, epoch, &apos;accuracy:&apos;, sess.run(accuracy, feed_dict=&#123;keep_prob:1.0, xs: mnist.test.images, ys: mnist.test.labels&#125;)) end = time.time() print (&apos;*&apos; * 30) print (&apos;training finish. cost time:&apos;, int(end-st) , &apos;seconds; accuracy:&apos;, sess.run(accuracy, feed_dict=&#123;keep_prob:1.0, xs: mnist.test.images, ys: mnist.test.labels&#125;))","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"https://github.com/LiHuaBai/categories/DeepLearning/"},{"name":"tensorflow","slug":"DeepLearning/tensorflow","permalink":"https://github.com/LiHuaBai/categories/DeepLearning/tensorflow/"}],"tags":[]},{"title":"tensorflow学习笔记（二）——数据类型","slug":"tf2","date":"2017-08-16T16:00:00.000Z","updated":"2019-07-08T12:25:32.846Z","comments":true,"path":"2017/08/17/tf2/","link":"","permalink":"https://github.com/LiHuaBai/2017/08/17/tf2/","excerpt":"看了官网的教程，先跑了第一个例程，大概学习了tensorflow的几种数据类型以及程序流程","text":"看了官网的教程，先跑了第一个例程，大概学习了tensorflow的几种数据类型以及程序流程 数据类型tensorflow的数据以张量的形式构成，其形状和类型的设定和实际数值的写入是分开的，主要有常量，变量和占位符三种类型：这是基本的python数组12# 普通的python数组a = [[1., 2., 3.], [4., 5., 6.]] 对于tensorflow常量1234567# tensorflow常量的赋值只能为定值，dtype定义了其数据为浮点型,name为其命名b1 = tf.constant(3,dtype=tf.float32,name = &apos;b1&apos;)b2 = tf.constant(a,dtype=tf.float32)# 也可用b1 = tf.constant(3,&quot;float&quot;)定义# 测试后发现tf.zeros([5]),tf.ones([5,5]),tf.random_uniform([5, 5], -2.0, 2.0)创建的也为常量数据# 其实质和constant相同，zeros等为常量的名称# 我理解为constant常量可用于给变量或占位符进行赋值 对于tensorflow变量12345678c1 = tf.Variable(a, dtype = tf.float32,name = &apos;c1&apos;)c2 = tf.Variable(b1, dtype = tf.float32)c3 = tf.Variable(tf.ones([5,5]))c4 = tf.Variable(tf.random_uniform([5, 5], -2.0, 2.0))# tensorflow变量可直接使用python普通数组赋值，也可用tensorflow常量# 若所赋的初始化值已定义数据类型，则可以不再定义# 使用变量需先初始化，在计算中其值是不断变换的，一般形状大小设置固定，用作被训练的权重等 对于tensorflow占位符12345678# 占位符是事先设定好，用来接受外部输入的一个值，一般用作训练样本和标签的数据# shape定义的形状大小，None代表任意大小，也可先不定义，与输入数据相同x1 = tf.placeholder(tf.float32)x2 = tf.placeholder(tf.float32,[None,3])x3 = tf.placeholder(dtype = tf.float32,shape = (2,2))# 在进行运算时，将数据从外部输入print(sess.run([x1,x2,x3], feed_dict = &#123;x1:[[1,2],[1,2],[3,4]],x2:[[1,2,3],[2,6,4]],x3:[[1,2],[3,4]]&#125;)) 完整代码123456789101112131415161718192021import tensorflow as tfa = [[1., 2., 3.], [4., 5., 6.]]b1 = tf.constant(3,dtype=tf.float32,name = &apos;b1&apos;)b2 = tf.constant(a,dtype=tf.float32)print(b1)print(b2)c1 = tf.Variable(a, dtype = tf.float32,name = &apos;c1&apos;)c2 = tf.Variable(b1, dtype = tf.float32)c3 = tf.Variable(tf.ones([5,5]))c4 = tf.Variable(tf.random_uniform([5, 5], -2.0, 2.0))x1 = tf.placeholder(tf.float32)x2 = tf.placeholder(tf.float32,[None,3])x3 = tf.placeholder(dtype = tf.float32,shape = (2,2))sess = tf.Session()# 创建图init = tf.global_variables_initializer()sess.run(init)# 将所有变量初始化print(sess.run([b1,b2]))print(sess.run([c1,c2,c3,c4]))print(sess.run([x1,x2,x3], feed_dict = &#123;x1:[[1,2],[1,2],[3,4]],x2:[[1,2,3],[2,6,4]],x3:[[1,2],[3,4]]&#125;))","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"https://github.com/LiHuaBai/categories/DeepLearning/"},{"name":"tensorflow","slug":"DeepLearning/tensorflow","permalink":"https://github.com/LiHuaBai/categories/DeepLearning/tensorflow/"}],"tags":[]},{"title":"写在最开始","slug":"firstblog","date":"2017-08-01T16:00:00.000Z","updated":"2019-07-08T12:25:32.844Z","comments":true,"path":"2017/08/02/firstblog/","link":"","permalink":"https://github.com/LiHuaBai/2017/08/02/firstblog/","excerpt":"博客终于搭建好了，也算是难得完成了一件想要做的事，希望接下来能完成更多的目标吧。","text":"博客终于搭建好了，也算是难得完成了一件想要做的事，希望接下来能完成更多的目标吧。 关于 我我是一个正在杭州中国计量大学的本科学生，自认为对编程还算比较有兴趣，最终还是选择了进入计算机专业学习。在学校的这两年，带着美好的幻想去尝试过许多东西，似乎每次在开始的时候还是能有一定的进展，最终却总是不能达到最初想要的结果，可能自己还是太菜了吧。 关于 博客说起来，想要有一个自己的博客已经很久了，却发现总是有各种事情要忙，导致把这个博客的搭建一直拖到了这个暑假。 一开始呢，是想着自己来写代码实现一个博客的，为此决定去学了JavaScript（放弃了PHP和Java），同样，一开始还是很认真的，从JS的基础语法，到ES6标准，对异步操作纠结了很久，前端选择了Vue框架，后端也学了nodeJS。然而学了一阵以后，发现好像事情没有那么简单，无论是Vue还是node，真要学好的话，这个坑还是有点深。预估了下学这些所需的时间，并且面前又出现了别的选项。于是还是放弃了这个想法。 最终我还是选用了hexo来快速搭建一个博客，按照教程，稍微学下git的用法，还是很快就完成了。这让我不得不说，开源的力量还是很大的。 关于 未来曾经花了不少精力在ACM的学习与参赛上，然而也没能获得令自己满意的成果，也参加了创业类的项目，过程与结果也都不尽如人意。有一阵在学Web开发的内容，发现和很早就一心做Web的同学相比还是欠缺不少，还是没能说服自己就踏实地做Web开发就好了，可能是脑子里还是有一些不切实际的幻想吧，于是在近期又参加了数学建模的比赛，虽然说不好又会翻车。 想了想，关于未来，似乎还是有许多空白的地方，那就慢慢来吧。目前决定去学习算法方面的内容，就目前来看，机器学习还是比较热门的，个人也有些兴趣。虽然也知道，以后可能要为一时的冲动买单。不过反正也不是第一次了，想要就去做嘛，重要的不是能有什么好处或者非要取得什么成就，而是能真正专心的做些东西吧。 既然已经有了自己的博客，希望之后能时常更博，将所学到的内容记录下来，并且能逐个完成自己所定的目标吧。","categories":[{"name":"日常","slug":"日常","permalink":"https://github.com/LiHuaBai/categories/日常/"}],"tags":[]}]}